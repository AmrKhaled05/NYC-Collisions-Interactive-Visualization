{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Data Integration: - Crashes & Person Datasets\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load cleaned datasets\n",
    "print(\"\\n LOADING CLEANED DATASETS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Load cleaned crashes data\n",
    "#crashes_cleaned = pd.read_csv('data/crashes_cleaned.csv', low_memory=False)\n",
    "crashes_cleaned = pd.read_csv('data_cleaned/nyc_vehicle_crashes_cleaned.csv', low_memory=False)\n",
    "print(f\"✓ Crashes (cleaned): {crashes_cleaned.shape}\")\n",
    "print(f\"  Columns: {crashes_cleaned.columns.tolist()}\")\n",
    "\n",
    "# Load cleaned person data\n",
    "#person_cleaned = pd.read_csv('data/person_cleaned.csv', low_memory=False)\n",
    "person_cleaned = pd.read_csv('data_cleaned/person_cleaned.csv', low_memory=False)\n",
    "print(f\"\\n✓ Person (cleaned): {person_cleaned.shape}\")\n",
    "print(f\"  Columns: {person_cleaned.columns.tolist()}\")\n",
    "\n",
    "# Display basic info\n",
    "print(f\"\\n DATASET OVERVIEW:\")\n",
    "print(f\"  Crashes: {len(crashes_cleaned):,} collision records\")\n",
    "print(f\"  Person: {len(person_cleaned):,} person records (occupants, pedestrians, cyclists)\")"
   ],
   "id": "411598442491e7ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1- Pre-integration analysis: Check COLLISION_ID coverage\n",
    "print(\"\\n PRE-INTEGRATION ANALYSIS: COLLISION_ID COVERAGE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check for missing COLLISION_ID in both datasets\n",
    "crashes_missing_id = crashes_cleaned['COLLISION_ID'].isna().sum()\n",
    "person_missing_id = person_cleaned['COLLISION_ID'].isna().sum()\n",
    "\n",
    "print(f\"\\n1. Missing COLLISION_ID:\")\n",
    "print(f\"   • Crashes dataset: {crashes_missing_id:,} ({crashes_missing_id / len(crashes_cleaned) * 100:.2f}%)\")\n",
    "print(f\"   • Person dataset: {person_missing_id:,} ({person_missing_id / len(person_cleaned) * 100:.2f}%)\")\n",
    "\n",
    "# Check unique COLLISION_IDs\n",
    "crashes_unique_ids = crashes_cleaned['COLLISION_ID'].nunique()\n",
    "person_unique_ids = person_cleaned['COLLISION_ID'].nunique()\n",
    "\n",
    "print(f\"\\n2. Unique COLLISION_IDs:\")\n",
    "print(f\"   • Crashes dataset: {crashes_unique_ids:,}\")\n",
    "print(f\"   • Person dataset: {person_unique_ids:,}\")\n",
    "\n",
    "# Find common and unique IDs\n",
    "crashes_ids = set(crashes_cleaned['COLLISION_ID'].dropna())\n",
    "person_ids = set(person_cleaned['COLLISION_ID'].dropna())\n",
    "\n",
    "common_ids = crashes_ids.intersection(person_ids)\n",
    "crashes_only = crashes_ids - person_ids\n",
    "person_only = person_ids - crashes_ids\n",
    "\n",
    "print(f\"\\n3. COLLISION_ID Overlap:\")\n",
    "print(f\"   • Common IDs (in both datasets): {len(common_ids):,}\")\n",
    "print(f\"   • IDs only in Crashes: {len(crashes_only):,}\")\n",
    "print(f\"   • IDs only in Person: {len(person_only):,}\")\n",
    "print(f\"   • Match rate: {len(common_ids) / len(crashes_ids) * 100:.2f}%\")\n",
    "\n",
    "# Analyze person records per collision\n",
    "print(f\"\\n4. Person Records per Collision:\")\n",
    "persons_per_collision = person_cleaned.groupby('COLLISION_ID').size()\n",
    "print(f\"   • Average: {persons_per_collision.mean():.2f} persons/collision\")\n",
    "print(f\"   • Median: {persons_per_collision.median():.0f} persons/collision\")\n",
    "print(f\"   • Max: {persons_per_collision.max():.0f} persons/collision\")\n",
    "print(f\"   • Distribution:\")\n",
    "distribution = persons_per_collision.value_counts().sort_index().head(10)\n",
    "for count, freq in distribution.items():\n",
    "    print(f\"     - {count} person(s): {freq:,} collisions ({freq / len(persons_per_collision) * 100:.1f}%)\")"
   ],
   "id": "c43e2d5252fe35e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3- Perform the integration\n",
    "print(\"\\n PERFORMING DATA INTEGRATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check if COLLISION_ID exists and has correct data type\n",
    "print(\"Pre-merge data type check:\")\n",
    "print(f\"  Crashes COLLISION_ID dtype: {crashes_cleaned['COLLISION_ID'].dtype}\")\n",
    "print(f\"  Person COLLISION_ID dtype: {person_cleaned['COLLISION_ID'].dtype}\")\n",
    "\n",
    "# Ensure COLLISION_ID is same type (convert to int64 where possible)\n",
    "crashes_cleaned['COLLISION_ID'] = pd.to_numeric(crashes_cleaned['COLLISION_ID'], errors='coerce').astype('Int64')\n",
    "person_cleaned['COLLISION_ID'] = pd.to_numeric(person_cleaned['COLLISION_ID'], errors='coerce').astype('Int64')\n",
    "\n",
    "# Perform INNER JOIN\n",
    "print(\"\\nExecuting INNER JOIN...\")\n",
    "start_time = pd.Timestamp.now()\n",
    "\n",
    "integrated_data = crashes_cleaned.merge(\n",
    "    person_cleaned,\n",
    "    on='COLLISION_ID',\n",
    "    how='inner',\n",
    "    suffixes=('_CRASH', '_PERSON'),\n",
    "    indicator=True\n",
    ")\n",
    "\n",
    "end_time = pd.Timestamp.now()\n",
    "merge_duration = (end_time - start_time).total_seconds()\n",
    "\n",
    "print(f\"✓ Join completed in {merge_duration:.2f} seconds\")\n",
    "print(f\"\\n INTEGRATION RESULTS:\")\n",
    "print(f\"   • Original crashes: {len(crashes_cleaned):,} rows\")\n",
    "print(f\"   • Original persons: {len(person_cleaned):,} rows\")\n",
    "print(f\"   • Integrated dataset: {len(integrated_data):,} rows\")\n",
    "print(f\"   • Expansion factor: {len(integrated_data) / len(crashes_cleaned):.2f}x\")\n",
    "\n",
    "# Analyze merge indicator\n",
    "merge_stats = integrated_data['_merge'].value_counts()\n",
    "print(f\"\\n MERGE STATISTICS:\")\n",
    "for merge_type, count in merge_stats.items():\n",
    "    percentage = count / len(integrated_data) * 100\n",
    "    if merge_type == 'both':\n",
    "        print(f\"   • Matched (crash + person data): {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "# Drop the merge indicator column\n",
    "integrated_data = integrated_data.drop('_merge', axis=1)"
   ],
   "id": "bdfefd7b31a90f0b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "integrated_data.drop(columns=['CRASH_DATETIME'],inplace=True)",
   "id": "7145222633dfcafe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "integrated_data.info()",
   "id": "de2d7e9299c1239e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4- Post-integration data quality assessment\n",
    "print(\"\\n POST-INTEGRATION DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check for duplicate columns (from suffixes)\n",
    "duplicate_cols = [col for col in integrated_data.columns if '_CRASH' in col or '_PERSON' in col]\n",
    "if len(duplicate_cols) > 0:\n",
    "    print(f\"\\n DUPLICATE COLUMNS DETECTED (from merge suffixes):\")\n",
    "    for col in sorted(duplicate_cols):\n",
    "        print(f\"   • {col}\")\n",
    "    print(f\"\\n   Action required: Resolve {len(duplicate_cols)} duplicate columns\")\n",
    "else:\n",
    "    print(\"\\n✓ No duplicate columns detected\")\n",
    "\n",
    "# Check for new missing values\n",
    "print(f\"\\n MISSING VALUES IN PERSON COLUMNS:\")\n",
    "person_cols = [col for col in integrated_data.columns if col in person_cleaned.columns and col != 'COLLISION_ID']\n",
    "missing_summary = []\n",
    "for col in person_cols[:10]:  # Show first 10 person columns\n",
    "    missing_count = integrated_data[col].isna().sum()\n",
    "    missing_pct = missing_count / len(integrated_data) * 100\n",
    "    missing_summary.append({\n",
    "        'Column': col,\n",
    "        'Missing': missing_count,\n",
    "        'Percentage': f\"{missing_pct:.1f}%\"\n",
    "    })\n",
    "\n",
    "missing_df = pd.DataFrame(missing_summary)\n",
    "print(missing_df.to_string(index=False))\n",
    "\n",
    "# Data type consistency check\n",
    "print(f\"\\n DATA TYPE CONSISTENCY:\")\n",
    "print(f\"   • Total columns: {len(integrated_data.columns)}\")\n",
    "print(f\"   • Numeric columns: {len(integrated_data.select_dtypes(include=[np.number]).columns)}\")\n",
    "print(f\"   • Object/String columns: {len(integrated_data.select_dtypes(include=['object']).columns)}\")\n",
    "print(f\"   • Datetime columns: {len(integrated_data.select_dtypes(include=['datetime64']).columns)}\")"
   ],
   "id": "a14bbe31287aeec6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 7- Save integrated dataset\n",
    "print(\"\\n SAVING INTEGRATED DATASET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "output_path = 'integrated_crashes_person.csv'\n",
    "integrated_data.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"✓ Integrated dataset saved to: {output_path}\")\n",
    "print(f\"  • Rows: {len(integrated_data):,}\")\n",
    "print(f\"  • Columns: {len(integrated_data.columns)}\")"
   ],
   "id": "516ee830fdeafce6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "integrated_data.isnull().sum()",
   "id": "a294bb60064c66db",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
