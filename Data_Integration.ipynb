{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-21T11:49:28.733070Z",
     "start_time": "2025-11-21T11:49:28.728883Z"
    }
   },
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Data Integration: - Crashes & Person Datasets\")\n",
    "print(f\"\\nIntegration Start Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Integration: - Crashes & Person Datasets\n",
      "\n",
      "Integration Start Time: 2025-11-21 13:49:28\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T12:07:29.769073Z",
     "start_time": "2025-11-21T12:07:29.022065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load cleaned datasets\n",
    "print(\"\\n LOADING CLEANED DATASETS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Load cleaned crashes data\n",
    "#crashes_cleaned = pd.read_csv('data/crashes_cleaned.csv', low_memory=False)\n",
    "crashes_cleaned = pd.read_csv('data/Motor_Vehicle_Collisions_Crashes.csv', low_memory=False)\n",
    "print(f\"✓ Crashes (cleaned): {crashes_cleaned.shape}\")\n",
    "print(f\"  Columns: {crashes_cleaned.columns.tolist()}\")\n",
    "\n",
    "# Load cleaned person data\n",
    "#person_cleaned = pd.read_csv('data/person_cleaned.csv', low_memory=False)\n",
    "person_cleaned = pd.read_csv('data/Motor_Vehicle_Collisions_Person.csv.csv', low_memory=False)\n",
    "print(f\"\\n✓ Person (cleaned): {person_cleaned.shape}\")\n",
    "print(f\"  Columns: {person_cleaned.columns.tolist()}\")\n",
    "\n",
    "# Display basic info\n",
    "print(f\"\\n DATASET OVERVIEW:\")\n",
    "print(f\"  Crashes: {len(crashes_cleaned):,} collision records\")\n",
    "print(f\"  Person: {len(person_cleaned):,} person records (occupants, pedestrians, cyclists)\")"
   ],
   "id": "411598442491e7ba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " LOADING CLEANED DATASETS\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'crashes_cleaned' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 7\u001B[39m\n\u001B[32m      3\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m-\u001B[39m\u001B[33m\"\u001B[39m * \u001B[32m80\u001B[39m)\n\u001B[32m      5\u001B[39m \u001B[38;5;66;03m# Load cleaned crashes data\u001B[39;00m\n\u001B[32m      6\u001B[39m \u001B[38;5;66;03m#crashes_cleaned = pd.read_csv('data/crashes_cleaned.csv', low_memory=False)\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m✓ Crashes (cleaned): \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[43mcrashes_cleaned\u001B[49m.shape\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m      8\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m  Columns: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcrashes_cleaned.columns.tolist()\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     10\u001B[39m \u001B[38;5;66;03m# Load cleaned person data\u001B[39;00m\n",
      "\u001B[31mNameError\u001B[39m: name 'crashes_cleaned' is not defined"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1- Pre-integration analysis: Check COLLISION_ID coverage\n",
    "print(\"\\n PRE-INTEGRATION ANALYSIS: COLLISION_ID COVERAGE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check for missing COLLISION_ID in both datasets\n",
    "crashes_missing_id = crashes_cleaned['COLLISION_ID'].isna().sum()\n",
    "person_missing_id = person_cleaned['COLLISION_ID'].isna().sum()\n",
    "\n",
    "print(f\"\\n1. Missing COLLISION_ID:\")\n",
    "print(f\"   • Crashes dataset: {crashes_missing_id:,} ({crashes_missing_id / len(crashes_cleaned) * 100:.2f}%)\")\n",
    "print(f\"   • Person dataset: {person_missing_id:,} ({person_missing_id / len(person_cleaned) * 100:.2f}%)\")\n",
    "\n",
    "# Check unique COLLISION_IDs\n",
    "crashes_unique_ids = crashes_cleaned['COLLISION_ID'].nunique()\n",
    "person_unique_ids = person_cleaned['COLLISION_ID'].nunique()\n",
    "\n",
    "print(f\"\\n2. Unique COLLISION_IDs:\")\n",
    "print(f\"   • Crashes dataset: {crashes_unique_ids:,}\")\n",
    "print(f\"   • Person dataset: {person_unique_ids:,}\")\n",
    "\n",
    "# Find common and unique IDs\n",
    "crashes_ids = set(crashes_cleaned['COLLISION_ID'].dropna())\n",
    "person_ids = set(person_cleaned['COLLISION_ID'].dropna())\n",
    "\n",
    "common_ids = crashes_ids.intersection(person_ids)\n",
    "crashes_only = crashes_ids - person_ids\n",
    "person_only = person_ids - crashes_ids\n",
    "\n",
    "print(f\"\\n3. COLLISION_ID Overlap:\")\n",
    "print(f\"   • Common IDs (in both datasets): {len(common_ids):,}\")\n",
    "print(f\"   • IDs only in Crashes: {len(crashes_only):,}\")\n",
    "print(f\"   • IDs only in Person: {len(person_only):,}\")\n",
    "print(f\"   • Match rate: {len(common_ids) / len(crashes_ids) * 100:.2f}%\")\n",
    "\n",
    "# Analyze person records per collision\n",
    "print(f\"\\n4. Person Records per Collision:\")\n",
    "persons_per_collision = person_cleaned.groupby('COLLISION_ID').size()\n",
    "print(f\"   • Average: {persons_per_collision.mean():.2f} persons/collision\")\n",
    "print(f\"   • Median: {persons_per_collision.median():.0f} persons/collision\")\n",
    "print(f\"   • Max: {persons_per_collision.max():.0f} persons/collision\")\n",
    "print(f\"   • Distribution:\")\n",
    "distribution = persons_per_collision.value_counts().sort_index().head(10)\n",
    "for count, freq in distribution.items():\n",
    "    print(f\"     - {count} person(s): {freq:,} collisions ({freq / len(persons_per_collision) * 100:.1f}%)\")"
   ],
   "id": "c43e2d5252fe35e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 2- Decision point: Choose integration strategy\n",
    "print(\"\\n INTEGRATION STRATEGY SELECTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "STRATEGY OPTIONS:\n",
    "1. LEFT JOIN (Crashes ← Person): Keep all crashes, add person details\n",
    "2. INNER JOIN (Crashes ↔ Person): Keep only collisions with person data\n",
    "3. ONE-TO-MANY JOIN: Each crash can have multiple person records\n",
    "\n",
    "CHOSEN STRATEGY: LEFT JOIN (Option 1)\n",
    "JUSTIFICATION:\n",
    "  ✓ Preserves all collision records from crashes dataset\n",
    "  ✓ Maintains complete crash history even without person details\n",
    "  ✓ Allows analysis of crashes with/without reported injuries\n",
    "  ✓ Handles one-to-many relationship (1 crash → multiple persons)\n",
    "\n",
    "EXPECTED OUTCOME:\n",
    "  • Result will have MORE rows than crashes dataset\n",
    "  • Each crash with N persons → N rows in final dataset\n",
    "  • Crashes without person data → 1 row with NaN person fields\n",
    "\"\"\")"
   ],
   "id": "adad95f73282f7c5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 3- Perform the integration\n",
    "print(\"\\n PERFORMING DATA INTEGRATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check if COLLISION_ID exists and has correct data type\n",
    "print(\"Pre-merge data type check:\")\n",
    "print(f\"  Crashes COLLISION_ID dtype: {crashes_cleaned['COLLISION_ID'].dtype}\")\n",
    "print(f\"  Person COLLISION_ID dtype: {person_cleaned['COLLISION_ID'].dtype}\")\n",
    "\n",
    "# Ensure COLLISION_ID is same type (convert to int64 where possible)\n",
    "crashes_cleaned['COLLISION_ID'] = pd.to_numeric(crashes_cleaned['COLLISION_ID'], errors='coerce').astype('Int64')\n",
    "person_cleaned['COLLISION_ID'] = pd.to_numeric(person_cleaned['COLLISION_ID'], errors='coerce').astype('Int64')\n",
    "\n",
    "# Perform LEFT JOIN\n",
    "print(\"\\nExecuting LEFT JOIN...\")\n",
    "start_time = pd.Timestamp.now()\n",
    "\n",
    "integrated_data = crashes_cleaned.merge(\n",
    "    person_cleaned,\n",
    "    on='COLLISION_ID',\n",
    "    how='left',\n",
    "    suffixes=('_CRASH', '_PERSON'),\n",
    "    indicator=True\n",
    ")\n",
    "\n",
    "end_time = pd.Timestamp.now()\n",
    "merge_duration = (end_time - start_time).total_seconds()\n",
    "\n",
    "print(f\"✓ Join completed in {merge_duration:.2f} seconds\")\n",
    "print(f\"\\n INTEGRATION RESULTS:\")\n",
    "print(f\"   • Original crashes: {len(crashes_cleaned):,} rows\")\n",
    "print(f\"   • Original persons: {len(person_cleaned):,} rows\")\n",
    "print(f\"   • Integrated dataset: {len(integrated_data):,} rows\")\n",
    "print(f\"   • Expansion factor: {len(integrated_data) / len(crashes_cleaned):.2f}x\")\n",
    "\n",
    "# Analyze merge indicator\n",
    "merge_stats = integrated_data['_merge'].value_counts()\n",
    "print(f\"\\n MERGE STATISTICS:\")\n",
    "for merge_type, count in merge_stats.items():\n",
    "    percentage = count / len(integrated_data) * 100\n",
    "    if merge_type == 'both':\n",
    "        print(f\"   • Matched (crash + person data): {count:,} ({percentage:.1f}%)\")\n",
    "    elif merge_type == 'left_only':\n",
    "        print(f\"   • Crash only (no person data): {count:,} ({percentage:.1f}%)\")\n",
    "    elif merge_type == 'right_only':\n",
    "        print(f\"   • Person only (no crash data): {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "# Drop the merge indicator column\n",
    "integrated_data = integrated_data.drop('_merge', axis=1)"
   ],
   "id": "bdfefd7b31a90f0b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 4- Post-integration data quality assessment\n",
    "print(\"\\n POST-INTEGRATION DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check for duplicate columns (from suffixes)\n",
    "duplicate_cols = [col for col in integrated_data.columns if '_CRASH' in col or '_PERSON' in col]\n",
    "if len(duplicate_cols) > 0:\n",
    "    print(f\"\\n DUPLICATE COLUMNS DETECTED (from merge suffixes):\")\n",
    "    for col in sorted(duplicate_cols):\n",
    "        print(f\"   • {col}\")\n",
    "    print(f\"\\n   Action required: Resolve {len(duplicate_cols)} duplicate columns\")\n",
    "else:\n",
    "    print(\"\\n✓ No duplicate columns detected\")\n",
    "\n",
    "# Check for new missing values\n",
    "print(f\"\\n MISSING VALUES IN PERSON COLUMNS:\")\n",
    "person_cols = [col for col in integrated_data.columns if col in person_cleaned.columns and col != 'COLLISION_ID']\n",
    "missing_summary = []\n",
    "for col in person_cols[:10]:  # Show first 10 person columns\n",
    "    missing_count = integrated_data[col].isna().sum()\n",
    "    missing_pct = missing_count / len(integrated_data) * 100\n",
    "    missing_summary.append({\n",
    "        'Column': col,\n",
    "        'Missing': missing_count,\n",
    "        'Percentage': f\"{missing_pct:.1f}%\"\n",
    "    })\n",
    "\n",
    "missing_df = pd.DataFrame(missing_summary)\n",
    "print(missing_df.to_string(index=False))\n",
    "\n",
    "# Data type consistency check\n",
    "print(f\"\\n DATA TYPE CONSISTENCY:\")\n",
    "print(f\"   • Total columns: {len(integrated_data.columns)}\")\n",
    "print(f\"   • Numeric columns: {len(integrated_data.select_dtypes(include=[np.number]).columns)}\")\n",
    "print(f\"   • Object/String columns: {len(integrated_data.select_dtypes(include=['object']).columns)}\")\n",
    "print(f\"   • Datetime columns: {len(integrated_data.select_dtypes(include=['datetime64']).columns)}\")"
   ],
   "id": "a14bbe31287aeec6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 5- Handle CRASH_DATETIME conflict (if both datasets have datetime columns)\n",
    "print(\"\\n RESOLVING DATETIME COLUMN CONFLICTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check if we have duplicate datetime columns\n",
    "datetime_cols = [col for col in integrated_data.columns if 'DATETIME' in col.upper() or 'CRASH_DATE' in col.upper()]\n",
    "\n",
    "if len(datetime_cols) > 1:\n",
    "    print(f\"Found {len(datetime_cols)} datetime-related columns:\")\n",
    "    for col in datetime_cols:\n",
    "        print(f\"   • {col}: {integrated_data[col].dtype}\")\n",
    "\n",
    "    # Strategy: Keep CRASH_DATETIME from crashes dataset (more authoritative)\n",
    "    if 'CRASH_DATETIME_CRASH' in integrated_data.columns:\n",
    "        integrated_data['CRASH_DATETIME'] = integrated_data['CRASH_DATETIME_CRASH']\n",
    "        integrated_data = integrated_data.drop(['CRASH_DATETIME_CRASH', 'CRASH_DATETIME_PERSON'], axis=1,\n",
    "                                               errors='ignore')\n",
    "        print(f\"\\n✓ Resolved: Kept CRASH_DATETIME from crashes dataset\")\n",
    "    elif 'CRASH_DATETIME' in integrated_data.columns and integrated_data['CRASH_DATETIME'].notna().sum() > 0:\n",
    "        print(f\"\\n✓ No conflict: Single CRASH_DATETIME column already present\")\n",
    "else:\n",
    "    print(\"✓ No datetime column conflicts detected\")"
   ],
   "id": "e2bc0001e51fac77"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 6- Analyze integrated data relationships\n",
    "print(\"\\n ANALYZING CRASH-PERSON RELATIONSHIPS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Crashes with person data vs without\n",
    "crashes_with_persons = integrated_data[integrated_data['PERSON_TYPE'].notna()].groupby('COLLISION_ID').size()\n",
    "crashes_without_persons = len(crashes_cleaned) - len(crashes_with_persons)\n",
    "\n",
    "print(f\"1. Crashes by person data availability:\")\n",
    "print(\n",
    "    f\"   • With person data: {len(crashes_with_persons):,} ({len(crashes_with_persons) / len(crashes_cleaned) * 100:.1f}%)\")\n",
    "print(\n",
    "    f\"   • Without person data: {crashes_without_persons:,} ({crashes_without_persons / len(crashes_cleaned) * 100:.1f}%)\")\n",
    "\n",
    "# Person types distribution in integrated data\n",
    "if 'PERSON_TYPE' in integrated_data.columns:\n",
    "    print(f\"\\n2. Person types in integrated dataset:\")\n",
    "    person_types = integrated_data['PERSON_TYPE'].value_counts(dropna=False)\n",
    "    for ptype, count in person_types.items():\n",
    "        pct = count / len(integrated_data) * 100\n",
    "        print(f\"   • {ptype}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "# Injury analysis\n",
    "if 'PERSON_INJURY' in integrated_data.columns:\n",
    "    print(f\"\\n3. Injury distribution:\")\n",
    "    injuries = integrated_data['PERSON_INJURY'].value_counts(dropna=False)\n",
    "    for injury, count in injuries.head(5).items():\n",
    "        pct = count / len(integrated_data) * 100\n",
    "        print(f\"   • {injury}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "# Cross-validation: Compare injury counts\n",
    "if all(col in integrated_data.columns for col in ['NUMBER OF PERSONS INJURED', 'PERSON_INJURY']):\n",
    "    print(f\"\\n4. Cross-validation: Injury count consistency\")\n",
    "\n",
    "    # Count injured persons per collision in integrated data\n",
    "    injured_per_collision = integrated_data[integrated_data['PERSON_INJURY'] == 'Injured'].groupby(\n",
    "        'COLLISION_ID').size()\n",
    "\n",
    "    # Get crashes that have both metrics\n",
    "    sample_crashes = integrated_data[integrated_data['NUMBER OF PERSONS INJURED'].notna()].drop_duplicates(\n",
    "        'COLLISION_ID').head(5)\n",
    "\n",
    "    print(f\"   Sample comparison (first 5 crashes):\")\n",
    "    for idx, row in sample_crashes.iterrows():\n",
    "        collision_id = row['COLLISION_ID']\n",
    "        reported = row['NUMBER OF PERSONS INJURED']\n",
    "        actual = injured_per_collision.get(collision_id, 0)\n",
    "        match = \"✓\" if reported == actual else \"⚠\"\n",
    "        print(f\"   {match} Collision {collision_id}: Reported={reported}, Actual={actual}\")"
   ],
   "id": "efe56c7214e450e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 7- Save integrated dataset\n",
    "print(\"\\n SAVING INTEGRATED DATASET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "output_path = 'data/integrated_crashes_person.csv'\n",
    "integrated_data.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"✓ Integrated dataset saved to: {output_path}\")\n",
    "print(f\"  • Rows: {len(integrated_data):,}\")\n",
    "print(f\"  • Columns: {len(integrated_data.columns)}\")\n",
    "print(f\"  • File size: {pd.read_csv(output_path).memory_usage(deep=True).sum() / 1024 ** 2:.1f} MB\")\n",
    "\n",
    "# Create summary statistics\n",
    "summary = {\n",
    "    'Integration Date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'Crashes (original)': len(crashes_cleaned),\n",
    "    'Persons (original)': len(person_cleaned),\n",
    "    'Integrated Rows': len(integrated_data),\n",
    "    'Expansion Factor': f\"{len(integrated_data) / len(crashes_cleaned):.2f}x\",\n",
    "    'Crashes with Persons': len(crashes_with_persons),\n",
    "    'Crashes without Persons': crashes_without_persons,\n",
    "    'Match Rate': f\"{len(crashes_with_persons) / len(crashes_cleaned) * 100:.1f}%\"\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame([summary]).T\n",
    "summary_df.columns = ['Value']\n",
    "print(f\"\\n INTEGRATION SUMMARY:\")\n",
    "print(summary_df.to_string())\n",
    "\n",
    "print(\"\\n✅ DATA INTEGRATION COMPLETE!\")"
   ],
   "id": "516ee830fdeafce6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
